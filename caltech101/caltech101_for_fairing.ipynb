{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200216 21:57:31 config:123] Using preprocessor: <kubeflow.fairing.preprocessors.converted_notebook.ConvertNotebookPreprocessor object at 0x7f829405b550>\n",
      "[I 200216 21:57:31 config:125] Using builder: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f82940af358>\n",
      "[I 200216 21:57:31 config:127] Using deployer: <kubeflow.fairing.builders.append.append.AppendBuilder object at 0x7f82940af358>\n",
      "[W 200216 21:57:31 append:50] Building image using Append builder...\n",
      "[I 200216 21:57:31 base:105] Creating docker context: /tmp/fairing_context_gqf0szq6\n",
      "[I 200216 21:57:32 converted_notebook:127] Converting caltech101_for_fairing.ipynb to caltech101_for_fairing.py\n",
      "[I 200216 21:57:32 docker_creds_:234] Loading Docker credentials for repository 'brightfly/tf-fairing:2.0-gpu'\n",
      "[W 200216 21:57:34 append:54] Image successfully built in 2.235296613071114s.\n",
      "[W 200216 21:57:34 append:94] Pushing image kubeflow-registry.default.svc.cluster.local:30000/caltech-fairing-job:F4001E55...\n",
      "[I 200216 21:57:34 docker_creds_:234] Loading Docker credentials for repository 'kubeflow-registry.default.svc.cluster.local:30000/caltech-fairing-job:F4001E55'\n",
      "[W 200216 21:57:34 append:81] Uploading kubeflow-registry.default.svc.cluster.local:30000/caltech-fairing-job:F4001E55\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:35c102085707f703de2d9eaad8752d6fe1b8f02b5d2149f1d8357c9cc7fb7d0a pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:d701a76e3193731210c61c838de0c3d8fdc8048b613ca88a58e11dc3223221ec pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:308e2d038b5334fb2ec9872230cfabd52b26102f6c8e0e257f3dcd63cc3b7177 pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:8e829fe70a46e3ac4334823560e98b257234c23629f19f05460e21a453091e6d pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:6bd87fc350bdace1c19bb54f401192524dabadd1ff536af72b59d0e70c5a6bbb pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:9f0a21d58e5dce5512db6d5595c6e9c4ab014917cf0644e2d282b8f5e3f2522a pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:071fd910512489f1cf72f43901d730bfcf302bd46b88924e81f401810bd0e4ce pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:6001e1789921cf851f6fb2e5fe05be70f482fe9c2286f66892fe5a3bc404569c pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:8810fcda1e6e2713f22a64b835ffa1ff15f49257f43dee869abef1929416d362 pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:cb558defce31ad81b8a57df3baf9648d4e86945f332ae7e69925bcdda867ba79 pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:d1555fb0eeaac2bbfe413314cbbed0dea65a0a72312f29154e21afda1972b0d1 pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:0586a82d62e69da8dab24b635faf9c5da803ff9d208111a25b0e2253c97ec331 pushed.\n",
      "[I 200216 21:57:35 docker_session_:284] Layer sha256:c73ad65f0763fc76b21c650ff9a5d7f4714c30a3dc27f462aa478c829a8efed1 pushed.\n",
      "[I 200216 21:57:36 docker_session_:284] Layer sha256:251f5509d51d9e4119d4ffb70d4820f8e2d7dc72ad15df3ebd7cd755539e40fd pushed.\n",
      "[I 200216 21:57:36 docker_session_:284] Layer sha256:c592fdfb0e51000077def684bbdd097d6d870e62ae94418ba81a3468b03ec833 pushed.\n",
      "[I 200216 21:57:36 docker_session_:284] Layer sha256:db108ae5c97b9d9b4a6ab30f08b9f00de1a383adb8b2fbdad530e801661c12c5 pushed.\n",
      "[I 200216 21:57:36 docker_session_:284] Layer sha256:57418a6dc9496eeb6c9324e7bdf6fd48b9e1ac25cb473bf7b71f40d44e2f1e64 pushed.\n",
      "[I 200216 21:57:37 docker_session_:284] Layer sha256:d506ca482a49ec50dabf3d6b43ad6ad14074d7b25b013a122945d81a3fb06746 pushed.\n",
      "[I 200216 21:57:37 docker_session_:284] Layer sha256:947e0f532378ce4f91ff44af563f21f5679d39a28efa2541594dd3f96730edb0 pushed.\n",
      "[I 200216 21:57:37 docker_session_:284] Layer sha256:f6490e4900cb64767e994addad4a4329cca4ccfce37a63d21249d89b63ceb478 pushed.\n",
      "[I 200216 21:57:37 docker_session_:284] Layer sha256:c4e2f5cde1e102db0e852d5e60ceac5bf377621b397ef3cb7bf1587052ea9520 pushed.\n",
      "[I 200216 21:57:37 docker_session_:284] Layer sha256:180c1b9ae1a5a2a4aba2e08646155b250793be7a97a21556f2c4383f35510f14 pushed.\n",
      "[I 200216 21:57:38 docker_session_:284] Layer sha256:034db42ae0fa9a91805c16e78f4a6a9d8f5826c95c92713f835b2866333c786c pushed.\n",
      "[I 200216 21:57:38 docker_session_:284] Layer sha256:b0a15f35c371773fb2237091edb340a15cf0e8615de8c9d8c4b8830c35a40265 pushed.\n",
      "[I 200216 21:57:38 docker_session_:284] Layer sha256:d12137d65eaa8a740f8bc9419a0fe3e1ba963478fb9179142935e67d667c6f55 pushed.\n",
      "[I 200216 21:57:39 docker_session_:284] Layer sha256:74a3420b0759a0426bc7a24daf48c0f376a7e8d293c015e0681884071c3ea0e1 pushed.\n",
      "[I 200216 21:57:42 docker_session_:284] Layer sha256:f9e7e7aaea7e58fd475e393007baf0251bad43cccaaef540a365ed1036173ae7 pushed.\n",
      "[I 200216 21:57:42 docker_session_:284] Layer sha256:7fb8ccf83688665b4137d132962349ffeae8bed7fee8f8b878fee4ecffe1f93d pushed.\n",
      "[I 200216 21:57:42 docker_session_:284] Layer sha256:7d830fc21c34e37fa1d2f683b3392ce76489814b1973691f0593b2ce945b2aca pushed.\n",
      "[I 200216 21:58:04 docker_session_:284] Layer sha256:06d09f7e28e650c4614bfeaa1336db8632aa02d48b9ff082d9cdf2edf4204376 pushed.\n",
      "[I 200216 21:58:06 docker_session_:284] Layer sha256:65881bda3d6dd3c81420e22d3f921824b72ae8c07137aed41d02c640a2f87f04 pushed.\n",
      "[I 200216 21:58:06 docker_session_:334] Finished upload of: kubeflow-registry.default.svc.cluster.local:30000/caltech-fairing-job:F4001E55\n",
      "[W 200216 21:58:06 append:99] Pushed image kubeflow-registry.default.svc.cluster.local:30000/caltech-fairing-job:F4001E55 in 32.675969483098015s.\n",
      "[W 200216 21:58:06 job:90] The job fairing-job-2d29j launched.\n",
      "[W 200216 21:58:07 manager:227] Waiting for fairing-job-2d29j-chs85 to start...\n",
      "[W 200216 21:58:07 manager:227] Waiting for fairing-job-2d29j-chs85 to start...\n",
      "[W 200216 21:58:07 manager:227] Waiting for fairing-job-2d29j-chs85 to start...\n",
      "[W 200216 21:58:07 manager:227] Waiting for fairing-job-2d29j-chs85 to start...\n",
      "[I 200216 21:58:09 manager:233] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-16 21:58:12.143513: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-02-16 21:58:12.193978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.195116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-02-16 21:58:12.195325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.196181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:05.0\n",
      "2020-02-16 21:58:12.196544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-02-16 21:58:12.198641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2020-02-16 21:58:12.200460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2020-02-16 21:58:12.200845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2020-02-16 21:58:12.202851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2020-02-16 21:58:12.204336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2020-02-16 21:58:12.209469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-02-16 21:58:12.209606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.210640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.211587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.212825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.213710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\n",
      "2020-02-16 21:58:12.214076: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-02-16 21:58:12.226331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-02-16 21:58:12.226947: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x6572f20 executing computations on platform Host. Devices:\n",
      "2020-02-16 21:58:12.227001: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "2020-02-16 21:58:12.474435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.476378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.477583: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x65f5770 executing computations on platform CUDA. Devices:\n",
      "2020-02-16 21:58:12.477612: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2020-02-16 21:58:12.477619: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7\n",
      "2020-02-16 21:58:12.478212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.479186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "2020-02-16 21:58:12.479314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.480258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:05.0\n",
      "2020-02-16 21:58:12.480335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-02-16 21:58:12.480364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2020-02-16 21:58:12.480382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2020-02-16 21:58:12.480395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2020-02-16 21:58:12.480408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2020-02-16 21:58:12.480475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2020-02-16 21:58:12.480543: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-02-16 21:58:12.480637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.481672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.482613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.483755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.484710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\n",
      "2020-02-16 21:58:12.484784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2020-02-16 21:58:12.487354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-02-16 21:58:12.487391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1\n",
      "2020-02-16 21:58:12.487402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y\n",
      "2020-02-16 21:58:12.487410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N\n",
      "2020-02-16 21:58:12.487628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.488806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.489667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.490851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10804 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "2020-02-16 21:58:12.491617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-02-16 21:58:12.492585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10804 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      " 6488064/87910968 [>.............................] - ETA: 36s0\n",
      "26918912/87910968 [========>.....................] - ETA: 90\n",
      "59659904/87910968 [===============>..............] - ETA: \n",
      "69763072/87910968 [======================>.......] - ETA: \n",
      "87916544/87910968 [==============================] - 7s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to\n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200, 200, 3) 0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 99, 99, 32)   864         input_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 99, 99, 32)   96          conv2d[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 99, 99, 32)   0           batch_normalization[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 97, 97, 32)   9216        activation[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 97, 97, 32)   96          conv2d_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 97, 97, 32)   0           batch_normalization_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 97, 97, 64)   18432       activation_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 97, 97, 64)   192         conv2d_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 97, 97, 64)   0           batch_normalization_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 48, 48, 64)   0           activation_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 48, 48, 80)   5120        max_pooling2d[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 48, 80)   240         conv2d_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 48, 80)   0           batch_normalization_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 46, 46, 192)  138240      activation_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 46, 46, 192)  576         conv2d_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 46, 46, 192)  0           batch_normalization_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 192)  0           activation_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 22, 22, 64)   12288       max_pooling2d_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 22, 22, 64)   192         conv2d_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 22, 22, 64)   0           batch_normalization_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 22, 22, 48)   9216        max_pooling2d_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 22, 22, 96)   55296       activation_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 22, 22, 48)   144         conv2d_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 22, 22, 96)   288         conv2d_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 22, 22, 48)   0           batch_normalization_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 22, 22, 96)   0           batch_normalization_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 22, 22, 192)  0           max_pooling2d_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 22, 22, 64)   12288       max_pooling2d_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 22, 22, 64)   76800       activation_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 22, 22, 96)   82944       activation_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 22, 22, 32)   6144        average_pooling2d[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 22, 22, 64)   192         conv2d_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 22, 22, 64)   192         conv2d_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 22, 22, 96)   288         conv2d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 22, 22, 32)   96          conv2d_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 22, 22, 64)   0           batch_normalization_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 22, 22, 64)   0           batch_normalization_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 22, 22, 96)   0           batch_normalization_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 22, 22, 32)   0           batch_normalization_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 22, 22, 256)  0           activation_5[0][0]\n",
      "                                                                 activation_7[0][0]\n",
      "                                                                 activation_10[0][0]\n",
      "                                                                 activation_11[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 22, 22, 64)   16384       mixed0[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 22, 22, 64)   192         conv2d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 22, 22, 64)   0           batch_normalization_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 22, 22, 48)   12288       mixed0[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 22, 22, 96)   55296       activation_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 22, 22, 48)   144         conv2d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 22, 22, 96)   288         conv2d_16[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 22, 22, 48)   0           batch_normalization_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 22, 22, 96)   0           batch_normalization_16[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 22, 22, 256)  0           mixed0[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 22, 22, 64)   16384       mixed0[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 22, 22, 64)   76800       activation_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 22, 22, 96)   82944       activation_16[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 22, 22, 64)   16384       average_pooling2d_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 22, 22, 64)   192         conv2d_12[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 22, 22, 64)   192         conv2d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 22, 22, 96)   288         conv2d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 22, 22, 64)   192         conv2d_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 22, 22, 64)   0           batch_normalization_12[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 22, 22, 64)   0           batch_normalization_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 22, 22, 96)   0           batch_normalization_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 22, 22, 64)   0           batch_normalization_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 22, 22, 288)  0           activation_12[0][0]\n",
      "                                                                 activation_14[0][0]\n",
      "                                                                 activation_17[0][0]\n",
      "                                                                 activation_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 22, 22, 64)   18432       mixed1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 22, 22, 64)   192         conv2d_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 22, 22, 64)   0           batch_normalization_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 22, 22, 48)   13824       mixed1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 22, 22, 96)   55296       activation_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 22, 22, 48)   144         conv2d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 22, 22, 96)   288         conv2d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 22, 22, 48)   0           batch_normalization_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 22, 22, 96)   0           batch_normalization_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 22, 22, 288)  0           mixed1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 22, 22, 64)   18432       mixed1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 22, 22, 64)   76800       activation_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 22, 22, 96)   82944       activation_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 22, 22, 64)   18432       average_pooling2d_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 22, 22, 64)   192         conv2d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 22, 22, 64)   192         conv2d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 22, 22, 96)   288         conv2d_24[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 22, 22, 64)   192         conv2d_25[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 22, 22, 64)   0           batch_normalization_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 22, 22, 64)   0           batch_normalization_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 22, 22, 96)   0           batch_normalization_24[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 22, 22, 64)   0           batch_normalization_25[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 22, 22, 288)  0           activation_19[0][0]\n",
      "                                                                 activation_21[0][0]\n",
      "                                                                 activation_24[0][0]\n",
      "                                                                 activation_25[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 22, 22, 64)   18432       mixed2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 22, 22, 64)   192         conv2d_27[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 22, 22, 64)   0           batch_normalization_27[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 22, 22, 96)   55296       activation_27[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 22, 22, 96)   288         conv2d_28[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 22, 22, 96)   0           batch_normalization_28[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 10, 10, 384)  995328      mixed2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 10, 10, 96)   82944       activation_28[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 10, 10, 384)  1152        conv2d_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 10, 10, 96)   288         conv2d_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 10, 10, 384)  0           batch_normalization_26[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 10, 10, 96)   0           batch_normalization_29[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 288)  0           mixed2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 10, 10, 768)  0           activation_26[0][0]\n",
      "                                                                 activation_29[0][0]\n",
      "                                                                 max_pooling2d_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 10, 10, 128)  98304       mixed3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 10, 10, 128)  384         conv2d_34[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 10, 10, 128)  0           batch_normalization_34[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 10, 10, 128)  114688      activation_34[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 10, 10, 128)  384         conv2d_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 10, 10, 128)  0           batch_normalization_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 10, 10, 128)  98304       mixed3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 10, 10, 128)  114688      activation_35[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 10, 10, 128)  384         conv2d_31[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 10, 10, 128)  384         conv2d_36[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 10, 10, 128)  0           batch_normalization_31[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 10, 10, 128)  0           batch_normalization_36[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 10, 10, 128)  114688      activation_31[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 10, 10, 128)  114688      activation_36[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 10, 10, 128)  384         conv2d_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 10, 10, 128)  384         conv2d_37[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 10, 10, 128)  0           batch_normalization_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 10, 10, 128)  0           batch_normalization_37[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 10, 10, 768)  0           mixed3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 10, 10, 192)  147456      mixed3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 10, 10, 192)  172032      activation_32[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 10, 10, 192)  172032      activation_37[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 10, 10, 192)  147456      average_pooling2d_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 10, 10, 192)  576         conv2d_30[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 10, 10, 192)  576         conv2d_33[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 10, 10, 192)  576         conv2d_38[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 10, 10, 192)  576         conv2d_39[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 10, 10, 192)  0           batch_normalization_30[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 10, 10, 192)  0           batch_normalization_33[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 10, 10, 192)  0           batch_normalization_38[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 10, 10, 192)  0           batch_normalization_39[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 10, 10, 768)  0           activation_30[0][0]\n",
      "                                                                 activation_33[0][0]\n",
      "                                                                 activation_38[0][0]\n",
      "                                                                 activation_39[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 10, 10, 160)  122880      mixed4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 10, 10, 160)  480         conv2d_44[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 10, 10, 160)  0           batch_normalization_44[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 10, 10, 160)  179200      activation_44[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 10, 10, 160)  480         conv2d_45[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 10, 10, 160)  0           batch_normalization_45[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 10, 10, 160)  122880      mixed4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 10, 10, 160)  179200      activation_45[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 10, 10, 160)  480         conv2d_41[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 10, 10, 160)  480         conv2d_46[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 10, 10, 160)  0           batch_normalization_41[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 10, 10, 160)  0           batch_normalization_46[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 10, 10, 160)  179200      activation_41[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 10, 10, 160)  179200      activation_46[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 10, 10, 160)  480         conv2d_42[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 10, 10, 160)  480         conv2d_47[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 10, 10, 160)  0           batch_normalization_42[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 10, 10, 160)  0           batch_normalization_47[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 10, 10, 768)  0           mixed4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 10, 10, 192)  147456      mixed4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 10, 10, 192)  215040      activation_42[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 10, 10, 192)  215040      activation_47[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 10, 10, 192)  147456      average_pooling2d_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 10, 10, 192)  576         conv2d_40[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 10, 10, 192)  576         conv2d_43[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 10, 10, 192)  576         conv2d_48[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 10, 10, 192)  576         conv2d_49[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 10, 10, 192)  0           batch_normalization_40[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 10, 10, 192)  0           batch_normalization_43[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 10, 10, 192)  0           batch_normalization_48[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 10, 10, 192)  0           batch_normalization_49[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 10, 10, 768)  0           activation_40[0][0]\n",
      "                                                                 activation_43[0][0]\n",
      "                                                                 activation_48[0][0]\n",
      "                                                                 activation_49[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 10, 10, 160)  122880      mixed5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 10, 10, 160)  480         conv2d_54[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 10, 10, 160)  0           batch_normalization_54[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 10, 10, 160)  179200      activation_54[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 10, 10, 160)  480         conv2d_55[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 10, 10, 160)  0           batch_normalization_55[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 10, 10, 160)  122880      mixed5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 10, 10, 160)  179200      activation_55[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 10, 10, 160)  480         conv2d_51[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 10, 10, 160)  480         conv2d_56[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 10, 10, 160)  0           batch_normalization_51[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 10, 10, 160)  0           batch_normalization_56[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 10, 10, 160)  179200      activation_51[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 10, 10, 160)  179200      activation_56[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 10, 10, 160)  480         conv2d_52[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 10, 10, 160)  480         conv2d_57[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 10, 10, 160)  0           batch_normalization_52[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 10, 10, 160)  0           batch_normalization_57[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 10, 10, 768)  0           mixed5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 10, 10, 192)  147456      mixed5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 10, 10, 192)  215040      activation_52[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 10, 10, 192)  215040      activation_57[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 10, 10, 192)  147456      average_pooling2d_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 10, 10, 192)  576         conv2d_50[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 10, 10, 192)  576         conv2d_53[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 10, 10, 192)  576         conv2d_58[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 10, 10, 192)  576         conv2d_59[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 10, 10, 192)  0           batch_normalization_50[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 10, 10, 192)  0           batch_normalization_53[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 10, 10, 192)  0           batch_normalization_58[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 10, 10, 192)  0           batch_normalization_59[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 10, 10, 768)  0           activation_50[0][0]\n",
      "                                                                 activation_53[0][0]\n",
      "                                                                 activation_58[0][0]\n",
      "                                                                 activation_59[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 10, 10, 192)  147456      mixed6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 10, 10, 192)  576         conv2d_64[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 10, 10, 192)  0           batch_normalization_64[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 10, 10, 192)  258048      activation_64[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 10, 10, 192)  576         conv2d_65[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 10, 10, 192)  0           batch_normalization_65[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 10, 10, 192)  147456      mixed6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 10, 10, 192)  258048      activation_65[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 10, 10, 192)  576         conv2d_61[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 10, 10, 192)  576         conv2d_66[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 10, 10, 192)  0           batch_normalization_61[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 10, 10, 192)  0           batch_normalization_66[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 10, 10, 192)  258048      activation_61[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 10, 10, 192)  258048      activation_66[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 10, 10, 192)  576         conv2d_62[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 10, 10, 192)  576         conv2d_67[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 10, 10, 192)  0           batch_normalization_62[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 10, 10, 192)  0           batch_normalization_67[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 10, 10, 768)  0           mixed6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 10, 10, 192)  147456      mixed6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 10, 10, 192)  258048      activation_62[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 10, 10, 192)  258048      activation_67[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 10, 10, 192)  147456      average_pooling2d_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 10, 10, 192)  576         conv2d_60[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 10, 10, 192)  576         conv2d_63[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 10, 10, 192)  576         conv2d_68[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 10, 10, 192)  576         conv2d_69[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 10, 10, 192)  0           batch_normalization_60[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 10, 10, 192)  0           batch_normalization_63[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 10, 10, 192)  0           batch_normalization_68[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 10, 10, 192)  0           batch_normalization_69[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 10, 10, 768)  0           activation_60[0][0]\n",
      "                                                                 activation_63[0][0]\n",
      "                                                                 activation_68[0][0]\n",
      "                                                                 activation_69[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 10, 10, 192)  147456      mixed7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 10, 10, 192)  576         conv2d_72[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 10, 10, 192)  0           batch_normalization_72[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 10, 10, 192)  258048      activation_72[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 10, 10, 192)  576         conv2d_73[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 10, 10, 192)  0           batch_normalization_73[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 10, 10, 192)  147456      mixed7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 10, 10, 192)  258048      activation_73[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 10, 10, 192)  576         conv2d_70[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 10, 10, 192)  576         conv2d_74[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 10, 10, 192)  0           batch_normalization_70[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 10, 10, 192)  0           batch_normalization_74[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 4, 4, 320)    552960      activation_70[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 4, 4, 192)    331776      activation_74[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 4, 4, 320)    960         conv2d_71[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 4, 4, 192)    576         conv2d_75[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 4, 4, 320)    0           batch_normalization_71[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 4, 4, 192)    0           batch_normalization_75[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 768)    0           mixed7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 4, 4, 1280)   0           activation_71[0][0]\n",
      "                                                                 activation_75[0][0]\n",
      "                                                                 max_pooling2d_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 4, 4, 448)    573440      mixed8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 4, 448)    1344        conv2d_80[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 4, 448)    0           batch_normalization_80[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 4, 4, 384)    491520      mixed8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 4, 384)    1548288     activation_80[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 4, 384)    1152        conv2d_77[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 4, 384)    1152        conv2d_81[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 4, 384)    0           batch_normalization_77[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 4, 384)    0           batch_normalization_81[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 4, 4, 384)    442368      activation_77[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 4, 4, 384)    442368      activation_77[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 4, 384)    442368      activation_81[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 4, 384)    442368      activation_81[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 4, 4, 1280)   0           mixed8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 4, 4, 320)    409600      mixed8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 4, 384)    1152        conv2d_78[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 4, 384)    1152        conv2d_79[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 4, 384)    1152        conv2d_82[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 4, 384)    1152        conv2d_83[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 4, 192)    245760      average_pooling2d_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 4, 320)    960         conv2d_76[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 4, 384)    0           batch_normalization_78[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 4, 384)    0           batch_normalization_79[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 4, 384)    0           batch_normalization_82[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 4, 384)    0           batch_normalization_83[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 4, 192)    576         conv2d_84[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 4, 320)    0           batch_normalization_76[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 4, 4, 768)    0           activation_78[0][0]\n",
      "                                                                 activation_79[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 4, 4, 768)    0           activation_82[0][0]\n",
      "                                                                 activation_83[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 4, 192)    0           batch_normalization_84[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 4, 4, 2048)   0           activation_76[0][0]\n",
      "                                                                 mixed9_0[0][0]\n",
      "                                                                 concatenate[0][0]\n",
      "                                                                 activation_84[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 4, 448)    917504      mixed9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 4, 448)    1344        conv2d_89[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 4, 4, 448)    0           batch_normalization_89[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 4, 384)    786432      mixed9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 4, 384)    1548288     activation_89[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 4, 384)    1152        conv2d_86[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 4, 384)    1152        conv2d_90[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 4, 384)    0           batch_normalization_86[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 4, 384)    0           batch_normalization_90[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 4, 384)    442368      activation_86[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 4, 384)    442368      activation_86[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 4, 384)    442368      activation_90[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 4, 384)    442368      activation_90[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 4, 4, 2048)   0           mixed9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 4, 320)    655360      mixed9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 4, 384)    1152        conv2d_87[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 4, 384)    1152        conv2d_88[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 4, 384)    1152        conv2d_91[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 4, 384)    1152        conv2d_92[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 4, 192)    393216      average_pooling2d_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 4, 320)    960         conv2d_85[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 4, 384)    0           batch_normalization_87[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 4, 384)    0           batch_normalization_88[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 4, 4, 384)    0           batch_normalization_91[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 4, 4, 384)    0           batch_normalization_92[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 4, 4, 192)    576         conv2d_93[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 4, 320)    0           batch_normalization_85[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 4, 4, 768)    0           activation_87[0][0]\n",
      "                                                                 activation_88[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 768)    0           activation_91[0][0]\n",
      "                                                                 activation_92[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 4, 4, 192)    0           batch_normalization_93[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 4, 4, 2048)   0           activation_85[0][0]\n",
      "                                                                 mixed9_1[0][0]\n",
      "                                                                 concatenate_1[0][0]\n",
      "                                                                 activation_93[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d (GlobalMax (None, 2048)         0           mixed10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "fully (Dense)                   (None, 1024)         2098176     global_max_pooling2d[0][0]\n",
      "2020-02-16 21:58:28.696236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-02-16 21:58:30.065763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 1024)         4096        fully[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 1024)         0           batch_normalization_94[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          524800      activation_94[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 512)          2048        dense[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 512)          0           batch_normalization_95[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, 101)          51813       activation_95[0][0]\n",
      "==================================================================================================\n",
      "Total params: 24,483,717\n",
      "Trainable params: 2,677,861\n",
      "Non-trainable params: 21,805,856\n",
      "__________________________________________________________________________________________________\n",
      "Found 6982 images belonging to 101 classes.\n",
      "Found 1695 images belonging to 101 classes.\n",
      "Epoch 1/100\n",
      "  7/436 [..............................] - ETA: 6:11 - loss: 4.0708 - acc: 0.21875  \n",
      " 13/436 [..............................] - ETA: 4:26 - loss: 3.8543 - acc: 0.276\n",
      " 19/436 [>.............................] - ETA: 3:55 - loss: 3.6902 - acc: 0.286\n",
      " 26/436 [>.............................] - ETA: 3:37 - loss: 3.3974 - acc: 0.330\n",
      " 32/436 [=>............................] - ETA: 3:25 - loss: 3.2053 - acc: 0.366\n",
      " 38/436 [=>............................] - ETA: 3:15 - loss: 3.1423 - acc: 0.378\n",
      " 44/436 [==>...........................] - ETA: 3:10 - loss: 3.0302 - acc: 0.393\n",
      " 51/436 [==>...........................] - ETA: 3:04 - loss: 2.9249 - acc: 0.410\n",
      " 57/436 [==>...........................] - ETA: 2:59 - loss: 2.8049 - acc: 0.421\n",
      " 63/436 [===>..........................] - ETA: 2:54 - loss: 2.7662 - acc: 0.435\n",
      " 69/436 [===>..........................] - ETA: 2:50 - loss: 2.6911 - acc: 0.446\n",
      " 76/436 [====>.........................] - ETA: 2:46 - loss: 2.6257 - acc: 0.458\n",
      " 82/436 [====>.........................] - ETA: 2:41 - loss: 2.5077 - acc: 0.486\n",
      " 88/436 [=====>........................] - ETA: 2:38 - loss: 2.4198 - acc: 0.495\n",
      " 95/436 [=====>........................] - ETA: 2:34 - loss: 2.3518 - acc: 0.512\n",
      "101/436 [=====>........................] - ETA: 2:31 - loss: 2.2846 - acc: 0.524\n",
      "107/436 [======>.......................] - ETA: 2:28 - loss: 2.2315 - acc: 0.533\n",
      "113/436 [======>.......................] - ETA: 2:24 - loss: 2.1740 - acc: 0.541\n",
      "120/436 [=======>......................] - ETA: 2:21 - loss: 2.1226 - acc: 0.550\n",
      "126/436 [=======>......................] - ETA: 2:18 - loss: 2.0773 - acc: 0.556\n",
      "132/436 [========>.....................] - ETA: 2:15 - loss: 2.0318 - acc: 0.562\n",
      "139/436 [========>.....................] - ETA: 2:13 - loss: 1.9943 - acc: 0.572\n",
      "145/436 [========>.....................] - ETA: 2:09 - loss: 1.9590 - acc: 0.578\n",
      "151/436 [=========>....................] - ETA: 2:06 - loss: 1.9187 - acc: 0.585\n",
      "157/436 [=========>....................] - ETA: 2:03 - loss: 1.8926 - acc: 0.589\n",
      "164/436 [==========>...................] - ETA: 2:01 - loss: 1.8690 - acc: 0.592\n",
      "170/436 [==========>...................] - ETA: 1:57 - loss: 1.8302 - acc: 0.608\n",
      "176/436 [===========>..................] - ETA: 1:54 - loss: 1.8113 - acc: 0.602\n",
      "183/436 [===========>..................] - ETA: 1:52 - loss: 1.7862 - acc: 0.607\n",
      "189/436 [============>.................] - ETA: 1:48 - loss: 1.7678 - acc: 0.609\n",
      "195/436 [============>.................] - ETA: 1:46 - loss: 1.7455 - acc: 0.612\n",
      "201/436 [============>.................] - ETA: 1:43 - loss: 1.7248 - acc: 0.616\n",
      "208/436 [=============>................] - ETA: 1:40 - loss: 1.6973 - acc: 0.622\n",
      "214/436 [=============>................] - ETA: 1:37 - loss: 1.6700 - acc: 0.627\n",
      "220/436 [==============>...............] - ETA: 1:34 - loss: 1.6526 - acc: 0.631\n",
      "227/436 [==============>...............] - ETA: 1:32 - loss: 1.6367 - acc: 0.634\n",
      "233/436 [===============>..............] - ETA: 1:28 - loss: 1.6231 - acc: 0.637\n",
      "239/436 [===============>..............] - ETA: 1:26 - loss: 1.6054 - acc: 0.641\n",
      "245/436 [===============>..............] - ETA: 1:23 - loss: 1.5886 - acc: 0.643\n",
      "252/436 [================>.............] - ETA: 1:20 - loss: 1.5745 - acc: 0.645\n",
      "258/436 [================>.............] - ETA: 1:17 - loss: 1.5523 - acc: 0.648\n",
      "264/436 [=================>............] - ETA: 1:14 - loss: 1.5380 - acc: 0.651\n",
      "271/436 [=================>............] - ETA: 1:12 - loss: 1.5228 - acc: 0.655\n",
      "277/436 [==================>...........] - ETA: 1:09 - loss: 1.5084 - acc: 0.659\n",
      "283/436 [==================>...........] - ETA: 1:06 - loss: 1.4968 - acc: 0.660\n",
      "289/436 [==================>...........] - ETA: 1:03 - loss: 1.4837 - acc: 0.661\n",
      "296/436 [===================>..........] - ETA: 1:01 - loss: 1.4709 - acc: 0.663\n",
      "302/436 [===================>..........] - ETA: 58s - loss: 1.4563 - acc: 0.6633\n",
      "308/436 [====================>.........] - ETA: 55s - loss: 1.4440 - acc: 0.66\n",
      "315/436 [====================>.........] - ETA: 52s - loss: 1.4339 - acc: 0.66\n",
      "321/436 [=====================>........] - ETA: 49s - loss: 1.4202 - acc: 0.670\n",
      "327/436 [=====================>........] - ETA: 47s - loss: 1.4129 - acc: 0.671\n",
      "334/436 [=====================>........] - ETA: 44s - loss: 1.4042 - acc: 0.67\n",
      "340/436 [======================>.......] - ETA: 41s - loss: 1.3909 - acc: 0.67\n",
      "346/436 [======================>.......] - ETA: 39s - loss: 1.3865 - acc: 0.67\n",
      "353/436 [=======================>......] - ETA: 35s - loss: 1.3796 - acc: 0.67\n",
      "359/436 [=======================>......] - ETA: 33s - loss: 1.3662 - acc: 0.678\n",
      "366/436 [========================>.....] - ETA: 30s - loss: 1.3564 - acc: 0.68\n",
      "372/436 [========================>.....] - ETA: 27s - loss: 1.3462 - acc: 0.68\n",
      "378/436 [=========================>....] - ETA: 25s - loss: 1.3344 - acc: 0.68\n",
      "385/436 [=========================>....] - ETA: 22s - loss: 1.3250 - acc: 0.68\n",
      "391/436 [=========================>....] - ETA: 19s - loss: 1.3146 - acc: 0.688\n",
      "397/436 [==========================>...] - ETA: 16s - loss: 1.3031 - acc: 0.691\n",
      "404/436 [==========================>...] - ETA: 14s - loss: 1.2966 - acc: 0.69\n",
      "410/436 [===========================>..] - ETA: 11s - loss: 1.2870 - acc: 0.69\n",
      "416/436 [===========================>..] - ETA: 8s - loss: 1.2788 - acc: 0.6951\n",
      "423/436 [============================>.] - ETA: 5s - loss: 1.2704 - acc: 0.697\n",
      "429/436 [============================>.] - ETA: 3s - loss: 1.2657 - acc: 0.697\n",
      "436/436 [==============================] - 208s 477- loss: 1.2588 - acc: 0.699\n",
      "ms/step - loss: 1.2569 - acc: 0.6995 - val_loss: 1.4560 - val_acc: 0.7198\n",
      "Epoch 2/100\n",
      "  7/436 [..............................] - ETA: 3:03 - loss: 0.3961 - acc: 0.864\n",
      " 13/436 [..............................] - ETA: 3:02 - loss: 0.4610 - acc: 0.848\n",
      " 19/436 [>.............................] - ETA: 2:57 - loss: 0.5145 - acc: 0.836\n",
      " 26/436 [>.............................] - ETA: 2:54 - loss: 0.5197 - acc: 0.832\n",
      " 32/436 [=>............................] - ETA: 2:50 - loss: 0.5515 - acc: 0.828\n",
      " 38/436 [=>............................] - ETA: 2:47 - loss: 0.5439 - acc: 0.833\n",
      " 44/436 [==>...........................] - ETA: 2:44 - loss: 0.5378 - acc: 0.838\n",
      " 51/436 [==>...........................] - ETA: 2:42 - loss: 0.5354 - acc: 0.838\n",
      " 57/436 [==>...........................] - ETA: 2:39 - loss: 0.5275 - acc: 0.841\n",
      " 63/436 [===>..........................] - ETA: 2:37 - loss: 0.5328 - acc: 0.848\n",
      " 70/436 [===>..........................] - ETA: 2:35 - loss: 0.5530 - acc: 0.837\n",
      " 76/436 [====>.........................] - ETA: 2:32 - loss: 0.5430 - acc: 0.844\n",
      " 82/436 [====>.........................] - ETA: 2:29 - loss: 0.5399 - acc: 0.845\n",
      " 88/436 [=====>........................] - ETA: 2:27 - loss: 0.5425 - acc: 0.844\n",
      " 95/436 [=====>........................] - ETA: 2:24 - loss: 0.5361 - acc: 0.847\n",
      "101/436 [=====>........................] - ETA: 2:21 - loss: 0.5330 - acc: 0.847\n",
      "107/436 [======>.......................] - ETA: 2:18 - loss: 0.5342 - acc: 0.848\n",
      "114/436 [======>.......................] - ETA: 2:16 - loss: 0.5356 - acc: 0.846\n",
      "120/436 [=======>......................] - ETA: 2:13 - loss: 0.5317 - acc: 0.847\n",
      "126/436 [=======>......................] - ETA: 2:10 - loss: 0.5314 - acc: 0.846\n",
      "132/436 [========>.....................] - ETA: 2:08 - loss: 0.5289 - acc: 0.846\n",
      "139/436 [========>.....................] - ETA: 2:05 - loss: 0.5204 - acc: 0.849\n",
      "145/436 [========>.....................] - ETA: 2:02 - loss: 0.5191 - acc: 0.850\n",
      "151/436 [=========>....................] - ETA: 2:00 - loss: 0.5229 - acc: 0.851\n",
      "158/436 [=========>....................] - ETA: 1:57 - loss: 0.5112 - acc: 0.854\n",
      "164/436 [==========>...................] - ETA: 1:54 - loss: 0.5101 - acc: 0.855\n",
      "170/436 [==========>...................] - ETA: 1:52 - loss: 0.5059 - acc: 0.856\n",
      "176/436 [===========>..................] - ETA: 1:49 - loss: 0.4987 - acc: 0.859\n",
      "183/436 [===========>..................] - ETA: 1:47 - loss: 0.4987 - acc: 0.858\n",
      "189/436 [============>.................] - ETA: 1:44 - loss: 0.5009 - acc: 0.857\n",
      "195/436 [============>.................] - ETA: 1:41 - loss: 0.4980 - acc: 0.858\n",
      "202/436 [============>.................] - ETA: 1:39 - loss: 0.4935 - acc: 0.859\n",
      "208/436 [=============>................] - ETA: 1:36 - loss: 0.4935 - acc: 0.860\n",
      "214/436 [=============>................] - ETA: 1:33 - loss: 0.4953 - acc: 0.860\n",
      "220/436 [==============>...............] - ETA: 1:31 - loss: 0.4998 - acc: 0.859\n",
      "227/436 [==============>...............] - ETA: 1:28 - loss: 0.4990 - acc: 0.860\n",
      "233/436 [===============>..............] - ETA: 1:25 - loss: 0.4956 - acc: 0.860\n",
      "239/436 [===============>..............] - ETA: 1:23 - loss: 0.4925 - acc: 0.861\n",
      "245/436 [===============>..............] - ETA: 1:20 - loss: 0.4981 - acc: 0.860\n",
      "252/436 [================>.............] - ETA: 1:18 - loss: 0.4989 - acc: 0.858\n",
      "258/436 [================>.............] - ETA: 1:15 - loss: 0.5002 - acc: 0.859\n",
      "264/436 [=================>............] - ETA: 1:12 - loss: 0.4980 - acc: 0.859\n",
      "271/436 [=================>............] - ETA: 1:10 - loss: 0.5014 - acc: 0.858\n",
      "277/436 [==================>...........] - ETA: 1:07 - loss: 0.5042 - acc: 0.856\n",
      "283/436 [==================>...........] - ETA: 1:04 - loss: 0.5070 - acc: 0.856\n",
      "289/436 [==================>...........] - ETA: 1:02 - loss: 0.5076 - acc: 0.857\n",
      "296/436 [===================>..........] - ETA: 59s - loss: 0.5101 - acc: 0.8567\n",
      "302/436 [===================>..........] - ETA: 56s - loss: 0.5187 - acc: 0.854\n",
      "308/436 [====================>.........] - ETA: 54s - loss: 0.5181 - acc: 0.85\n",
      "315/436 [====================>.........] - ETA: 51s - loss: 0.5207 - acc: 0.85\n",
      "321/436 [=====================>........] - ETA: 48s - loss: 0.5218 - acc: 0.85\n",
      "328/436 [=====================>........] - ETA: 46s - loss: 0.5217 - acc: 0.85\n",
      "334/436 [=====================>........] - ETA: 43s - loss: 0.5249 - acc: 0.85\n",
      "340/436 [======================>.......] - ETA: 40s - loss: 0.5281 - acc: 0.850\n",
      "347/436 [======================>.......] - ETA: 38s - loss: 0.5291 - acc: 0.85\n",
      "353/436 [=======================>......] - ETA: 35s - loss: 0.5264 - acc: 0.85\n",
      "359/436 [=======================>......] - ETA: 32s - loss: 0.5287 - acc: 0.85\n",
      "366/436 [========================>.....] - ETA: 30s - loss: 0.5313 - acc: 0.84\n",
      "372/436 [========================>.....] - ETA: 27s - loss: 0.5344 - acc: 0.849\n",
      "378/436 [=========================>....] - ETA: 24s - loss: 0.5348 - acc: 0.850\n",
      "385/436 [=========================>....] - ETA: 21s - loss: 0.5354 - acc: 0.85\n",
      "391/436 [=========================>....] - ETA: 19s - loss: 0.5380 - acc: 0.84\n",
      "398/436 [==========================>...] - ETA: 16s - loss: 0.5371 - acc: 0.84\n",
      "404/436 [==========================>...] - ETA: 13s - loss: 0.5408 - acc: 0.84\n",
      "410/436 [===========================>..] - ETA: 10s - loss: 0.5379 - acc: 0.848\n",
      "417/436 [===========================>..] - ETA: 8s - loss: 0.5380 - acc: 0.8486\n",
      "423/436 [============================>.] - ETA: 5s - loss: 0.5403 - acc: 0.847\n",
      "430/436 [============================>.] - ETA: 2s - loss: 0.5415 - acc: 0.847\n",
      "436/436 [==============================] - 203s 465ms/step - loss: 0.5409 - ac\n",
      "c: 0.8477 - val_loss: 1.3988 - val_acc: 0.7239\n",
      "Epoch 3/100\n",
      "  7/436 [..............................] - ETA: 3:04 - loss: 0.2790 - acc: 0.916\n",
      " 13/436 [..............................] - ETA: 3:01 - loss: 0.4087 - acc: 0.870\n",
      " 19/436 [>.............................] - ETA: 2:56 - loss: 0.4129 - acc: 0.878\n",
      " 26/436 [>.............................] - ETA: 2:53 - loss: 0.3963 - acc: 0.885\n",
      " 32/436 [=>............................] - ETA: 2:49 - loss: 0.3941 - acc: 0.891\n",
      " 38/436 [=>............................] - ETA: 2:47 - loss: 0.3924 - acc: 0.891\n",
      " 44/436 [==>...........................] - ETA: 2:44 - loss: 0.3997 - acc: 0.889\n",
      " 51/436 [==>...........................] - ETA: 2:42 - loss: 0.3887 - acc: 0.891\n",
      " 57/436 [==>...........................] - ETA: 2:39 - loss: 0.3854 - acc: 0.899\n",
      " 63/436 [===>..........................] - ETA: 2:36 - loss: 0.3768 - acc: 0.892\n",
      " 70/436 [===>..........................] - ETA: 2:34 - loss: 0.3747 - acc: 0.891\n",
      " 76/436 [====>.........................] - ETA: 2:30 - loss: 0.3701 - acc: 0.892\n",
      " 82/436 [====>.........................] - ETA: 2:28 - loss: 0.3953 - acc: 0.886\n",
      " 88/436 [=====>........................] - ETA: 2:26 - loss: 0.3897 - acc: 0.887\n",
      " 95/436 [=====>........................] - ETA: 2:24 - loss: 0.3895 - acc: 0.888\n",
      "101/436 [=====>........................] - ETA: 2:21 - loss: 0.3985 - acc: 0.887\n",
      "107/436 [======>.......................] - ETA: 2:18 - loss: 0.4021 - acc: 0.885\n",
      "114/436 [======>.......................] - ETA: 2:16 - loss: 0.4091 - acc: 0.881\n",
      "120/436 [=======>......................] - ETA: 2:13 - loss: 0.4173 - acc: 0.879\n",
      "126/436 [=======>......................] - ETA: 2:10 - loss: 0.4156 - acc: 0.881\n",
      "132/436 [========>.....................] - ETA: 2:07 - loss: 0.4175 - acc: 0.877\n",
      "139/436 [========>.....................] - ETA: 2:05 - loss: 0.4194 - acc: 0.878\n",
      "145/436 [========>.....................] - ETA: 2:02 - loss: 0.4178 - acc: 0.878\n",
      "151/436 [=========>....................] - ETA: 1:59 - loss: 0.4173 - acc: 0.877\n",
      "158/436 [=========>....................] - ETA: 1:57 - loss: 0.4210 - acc: 0.876\n",
      "164/436 [==========>...................] - ETA: 1:54 - loss: 0.4255 - acc: 0.876\n",
      "170/436 [==========>...................] - ETA: 1:51 - loss: 0.4215 - acc: 0.877\n",
      "176/436 [===========>..................] - ETA: 1:49 - loss: 0.4162 - acc: 0.879\n",
      "183/436 [===========>..................] - ETA: 1:46 - loss: 0.4163 - acc: 0.880\n",
      "189/436 [============>.................] - ETA: 1:43 - loss: 0.4168 - acc: 0.879\n",
      "195/436 [============>.................] - ETA: 1:40 - loss: 0.4179 - acc: 0.879\n",
      "202/436 [============>.................] - ETA: 1:38 - loss: 0.4214 - acc: 0.878\n",
      "208/436 [=============>................] - ETA: 1:35 - loss: 0.4166 - acc: 0.878\n",
      "214/436 [=============>................] - ETA: 1:32 - loss: 0.4129 - acc: 0.879\n",
      "220/436 [==============>...............] - ETA: 1:30 - loss: 0.4119 - acc: 0.880\n",
      "227/436 [==============>...............] - ETA: 1:28 - loss: 0.4068 - acc: 0.882\n",
      "233/436 [===============>..............] - ETA: 1:25 - loss: 0.4116 - acc: 0.880\n",
      "239/436 [===============>..............] - ETA: 1:22 - loss: 0.4106 - acc: 0.880\n",
      "245/436 [===============>..............] - ETA: 1:20 - loss: 0.4126 - acc: 0.880\n",
      "252/436 [================>.............] - ETA: 1:17 - loss: 0.4092 - acc: 0.881\n",
      "258/436 [================>.............] - ETA: 1:14 - loss: 0.4135 - acc: 0.880\n",
      "264/436 [=================>............] - ETA: 1:12 - loss: 0.4139 - acc: 0.880\n",
      "271/436 [=================>............] - ETA: 1:09 - loss: 0.4172 - acc: 0.879\n",
      "277/436 [==================>...........] - ETA: 1:06 - loss: 0.4171 - acc: 0.879\n",
      "283/436 [==================>...........] - ETA: 1:04 - loss: 0.4165 - acc: 0.879\n",
      "289/436 [==================>...........] - ETA: 1:01 - loss: 0.4185 - acc: 0.878\n",
      "296/436 [===================>..........] - ETA: 59s - loss: 0.4202 - acc: 0.8788\n",
      "302/436 [===================>..........] - ETA: 56s - loss: 0.4207 - acc: 0.878\n",
      "308/436 [====================>.........] - ETA: 53s - loss: 0.4190 - acc: 0.87\n",
      "315/436 [====================>.........] - ETA: 50s - loss: 0.4191 - acc: 0.87\n",
      "321/436 [=====================>........] - ETA: 48s - loss: 0.4206 - acc: 0.87\n",
      "328/436 [=====================>........] - ETA: 45s - loss: 0.4197 - acc: 0.87\n",
      "334/436 [=====================>........] - ETA: 42s - loss: 0.4271 - acc: 0.87\n",
      "340/436 [======================>.......] - ETA: 40s - loss: 0.4245 - acc: 0.876\n",
      "347/436 [======================>.......] - ETA: 37s - loss: 0.4257 - acc: 0.87\n",
      "353/436 [=======================>......] - ETA: 34s - loss: 0.4242 - acc: 0.87\n",
      "359/436 [=======================>......] - ETA: 32s - loss: 0.4232 - acc: 0.87\n",
      "366/436 [========================>.....] - ETA: 29s - loss: 0.4212 - acc: 0.87\n",
      "372/436 [========================>.....] - ETA: 26s - loss: 0.4198 - acc: 0.879\n",
      "378/436 [=========================>....] - ETA: 24s - loss: 0.4196 - acc: 0.879\n",
      "385/436 [=========================>....] - ETA: 21s - loss: 0.4186 - acc: 0.87\n",
      "391/436 [=========================>....] - ETA: 18s - loss: 0.4211 - acc: 0.87\n",
      "398/436 [==========================>...] - ETA: 16s - loss: 0.4240 - acc: 0.87\n",
      "404/436 [==========================>...] - ETA: 13s - loss: 0.4220 - acc: 0.87\n",
      "410/436 [===========================>..] - ETA: 10s - loss: 0.4237 - acc: 0.878\n",
      "417/436 [===========================>..] - ETA: 8s - loss: 0.4233 - acc: 0.8783\n",
      "423/436 [============================>.] - ETA: 5s - loss: 0.4236 - acc: 0.878\n",
      "430/436 [============================>.] - ETA: 2s - loss: 0.4266 - acc: 0.877\n",
      "436/436 [==============================] - 201s 461ms/step - loss: 0.4280 - acc:\n",
      " 0.8767 - val_loss: 0.9287 - val_acc: 0.7870\n",
      "Epoch 4/100\n",
      "  7/436 [..............................] - ETA: 3:06 - loss: 0.3539 - acc: 0.906\n",
      " 13/436 [..............................] - ETA: 2:58 - loss: 0.3713 - acc: 0.891\n",
      " 19/436 [>.............................] - ETA: 2:54 - loss: 0.3391 - acc: 0.909\n",
      " 26/436 [>.............................] - ETA: 2:52 - loss: 0.3643 - acc: 0.892\n",
      " 32/436 [=>............................] - ETA: 2:50 - loss: 0.3532 - acc: 0.891\n",
      " 38/436 [=>............................] - ETA: 2:47 - loss: 0.3478 - acc: 0.893\n",
      " 44/436 [==>...........................] - ETA: 2:45 - loss: 0.3764 - acc: 0.883\n",
      " 51/436 [==>...........................] - ETA: 2:42 - loss: 0.3852 - acc: 0.881\n",
      " 57/436 [==>...........................] - ETA: 2:39 - loss: 0.3770 - acc: 0.881\n",
      " 63/436 [===>..........................] - ETA: 2:36 - loss: 0.3735 - acc: 0.885\n",
      " 70/436 [===>..........................] - ETA: 2:33 - loss: 0.3717 - acc: 0.887\n",
      " 76/436 [====>.........................] - ETA: 2:30 - loss: 0.3689 - acc: 0.891\n",
      " 82/436 [====>.........................] - ETA: 2:28 - loss: 0.3659 - acc: 0.890\n",
      " 88/436 [=====>........................] - ETA: 2:25 - loss: 0.3631 - acc: 0.887\n",
      " 95/436 [=====>........................] - ETA: 2:23 - loss: 0.3594 - acc: 0.888\n",
      "101/436 [=====>........................] - ETA: 2:20 - loss: 0.3610 - acc: 0.886\n",
      "107/436 [======>.......................] - ETA: 2:17 - loss: 0.3524 - acc: 0.890\n",
      "114/436 [======>.......................] - ETA: 2:15 - loss: 0.3592 - acc: 0.891\n",
      "120/436 [=======>......................] - ETA: 2:12 - loss: 0.3655 - acc: 0.889\n",
      "126/436 [=======>......................] - ETA: 2:09 - loss: 0.3686 - acc: 0.889\n",
      "132/436 [========>.....................] - ETA: 2:07 - loss: 0.3706 - acc: 0.889\n",
      "139/436 [========>.....................] - ETA: 2:04 - loss: 0.3668 - acc: 0.890\n",
      "145/436 [========>.....................] - ETA: 2:01 - loss: 0.3696 - acc: 0.889\n",
      "151/436 [=========>....................] - ETA: 1:59 - loss: 0.3743 - acc: 0.889\n",
      "158/436 [=========>....................] - ETA: 1:57 - loss: 0.3753 - acc: 0.889\n",
      "164/436 [==========>...................] - ETA: 1:54 - loss: 0.3838 - acc: 0.886\n",
      "170/436 [==========>...................] - ETA: 1:51 - loss: 0.3821 - acc: 0.884\n",
      "176/436 [===========>..................] - ETA: 1:49 - loss: 0.3859 - acc: 0.884\n",
      "183/436 [===========>..................] - ETA: 1:46 - loss: 0.3851 - acc: 0.883\n",
      "189/436 [============>.................] - ETA: 1:43 - loss: 0.3875 - acc: 0.883\n",
      "195/436 [============>.................] - ETA: 1:41 - loss: 0.3830 - acc: 0.884\n",
      "202/436 [============>.................] - ETA: 1:38 - loss: 0.3822 - acc: 0.884\n",
      "208/436 [=============>................] - ETA: 1:36 - loss: 0.3861 - acc: 0.884\n",
      "214/436 [=============>................] - ETA: 1:33 - loss: 0.3880 - acc: 0.882\n",
      "220/436 [==============>...............] - ETA: 1:30 - loss: 0.3896 - acc: 0.881\n",
      "227/436 [==============>...............] - ETA: 1:28 - loss: 0.3874 - acc: 0.883\n",
      "233/436 [===============>..............] - ETA: 1:25 - loss: 0.3908 - acc: 0.882\n",
      "239/436 [===============>..............] - ETA: 1:22 - loss: 0.3913 - acc: 0.882\n",
      "245/436 [===============>..............] - ETA: 1:20 - loss: 0.3860 - acc: 0.883\n",
      "252/436 [================>.............] - ETA: 1:17 - loss: 0.3869 - acc: 0.882\n",
      "258/436 [================>.............] - ETA: 1:14 - loss: 0.3886 - acc: 0.882\n",
      "264/436 [=================>............] - ETA: 1:12 - loss: 0.3873 - acc: 0.883\n",
      "271/436 [=================>............] - ETA: 1:09 - loss: 0.3842 - acc: 0.884\n",
      "277/436 [==================>...........] - ETA: 1:06 - loss: 0.3838 - acc: 0.884\n",
      "283/436 [==================>...........] - ETA: 1:04 - loss: 0.3828 - acc: 0.884\n",
      "289/436 [==================>...........] - ETA: 1:02 - loss: 0.3815 - acc: 0.884\n",
      "296/436 [===================>..........] - ETA: 59s - loss: 0.3857 - acc: 0.8833\n",
      "302/436 [===================>..........] - ETA: 56s - loss: 0.3844 - acc: 0.884\n",
      "308/436 [====================>.........] - ETA: 54s - loss: 0.3817 - acc: 0.88\n",
      "315/436 [====================>.........] - ETA: 51s - loss: 0.3809 - acc: 0.88\n",
      "321/436 [=====================>........] - ETA: 48s - loss: 0.3807 - acc: 0.88\n",
      "328/436 [=====================>........] - ETA: 46s - loss: 0.3813 - acc: 0.88\n",
      "334/436 [=====================>........] - ETA: 43s - loss: 0.3799 - acc: 0.88\n",
      "340/436 [======================>.......] - ETA: 40s - loss: 0.3804 - acc: 0.885\n",
      "347/436 [======================>.......] - ETA: 38s - loss: 0.3769 - acc: 0.88\n",
      "353/436 [=======================>......] - ETA: 35s - loss: 0.3759 - acc: 0.88\n",
      "359/436 [=======================>......] - ETA: 32s - loss: 0.3772 - acc: 0.88\n",
      "366/436 [========================>.....] - ETA: 29s - loss: 0.3754 - acc: 0.88\n",
      "372/436 [========================>.....] - ETA: 27s - loss: 0.3772 - acc: 0.885\n",
      "378/436 [=========================>....] - ETA: 24s - loss: 0.3762 - acc: 0.886\n",
      "385/436 [=========================>....] - ETA: 21s - loss: 0.3758 - acc: 0.88\n",
      "391/436 [=========================>....] - ETA: 18s - loss: 0.3761 - acc: 0.88\n",
      "398/436 [==========================>...] - ETA: 16s - loss: 0.3752 - acc: 0.88\n",
      "404/436 [==========================>...] - ETA: 13s - loss: 0.3746 - acc: 0.88\n",
      "410/436 [===========================>..] - ETA: 10s - loss: 0.3736 - acc: 0.886\n",
      "417/436 [===========================>..] - ETA: 8s - loss: 0.3720 - acc: 0.8879\n",
      "423/436 [============================>.] - ETA: 5s - loss: 0.3737 - acc: 0.887\n",
      "430/436 [============================>.] - ETA: 2s - loss: 0.3761 - acc: 0.886\n",
      "436/436 [==============================] - 202s 464ms/step - loss: 0.3805 - ac\n",
      "c: 0.8853 - val_loss: 1.0197 - val_acc: 0.7994\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 200216 22:12:04 job:162] Cleaning up job fairing-job-2d29j...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/436 [..............................] - ETA: 2:59 - loss: 0.2489 - acc: 0.8958\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Sun, 16 Feb 2020 22:12:04 GMT', 'Content-Length': '224'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"jobs.batch \\\"fairing-job-2d29j\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"fairing-job-2d29j\",\"group\":\"batch\",\"kind\":\"jobs\"},\"code\":404}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c53d3808abef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mipy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mfairing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mfairing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCaltech101\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubeflow/fairing/config.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mpod_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_pod_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mdeployer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpod_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeployer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubeflow/fairing/deployers/job/job.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, pod_spec)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubeflow/fairing/deployers/job/job.py\u001b[0m in \u001b[0;36mget_logs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubeflow/fairing/deployers/job/job.py\u001b[0m in \u001b[0;36mdo_cleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             body=k8s_client.V1DeleteOptions(propagation_policy='Foreground'))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/batch_v1_api.py\u001b[0m in \u001b[0;36mdelete_namespaced_job\u001b[0;34m(self, name, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_namespaced_job_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_namespaced_job_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/batch_v1_api.py\u001b[0m in \u001b[0;36mdelete_namespaced_job_with_http_info\u001b[0;34m(self, name, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m                                         \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_preload_content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                                         \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_request_timeout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                                         collection_formats=collection_formats)\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_api_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    332\u001b[0m                                    \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                                    \u001b[0mresponse_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                                    _return_http_data_only, collection_formats, _preload_content, _request_timeout)\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             thread = self.pool.apply_async(self.__call_api, (resource_path, method,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                      \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                                      \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                                      _request_timeout=_request_timeout)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    398\u001b[0m                                            \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                                            \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                                            body=body)\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py\u001b[0m in \u001b[0;36mDELETE\u001b[0;34m(self, url, headers, query_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                             body=body)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     def POST(self, url, headers=None, query_params=None, post_params=None, body=None, _preload_content=True,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: (404)\nReason: Not Found\nHTTP response headers: HTTPHeaderDict({'Content-Type': 'application/json', 'Date': 'Sun, 16 Feb 2020 22:12:04 GMT', 'Content-Length': '224'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"jobs.batch \\\"fairing-job-2d29j\\\" not found\",\"reason\":\"NotFound\",\"details\":{\"name\":\"fairing-job-2d29j\",\"group\":\"batch\",\"kind\":\"jobs\"},\"code\":404}\n\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Input, Activation, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "\n",
    "import os\n",
    "\n",
    "class Caltech101(object):\n",
    "    def run(self):\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        input = Input(shape=(200, 200, 3))\n",
    "        model = InceptionV3(input_tensor=input, include_top=False, weights='imagenet', pooling='max')\n",
    "\n",
    "        for layer in model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        input_image_size = (200, 200)\n",
    "\n",
    "        x = model.output\n",
    "        x = Dense(1024, name='fully')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(512)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dense(101, activation='softmax', name='softmax')(x)\n",
    "        model = Model(model.input, x)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        train_datagen = ImageDataGenerator(rescale=1. / 255, validation_split=0.2)\n",
    "        batch_size = 16\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            '/result/caltech101',\n",
    "            target_size=input_image_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training')\n",
    "\n",
    "        validation_generator = train_datagen.flow_from_directory(\n",
    "            '/result/caltech101',\n",
    "            target_size=input_image_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation')\n",
    "        \n",
    "        model = multi_gpu_model(model, gpus=2)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['acc'])\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=20, mode='auto', monitor='val_acc')\n",
    "        hist = model.fit_generator(train_generator,\n",
    "                                      steps_per_epoch=train_generator.samples // batch_size,\n",
    "                                      validation_data = validation_generator,\n",
    "                                      epochs=100,\n",
    "                                      callbacks=[early_stopping])\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    if os.getenv('FAIRING_RUNTIME', None) is None:\n",
    "        from kubeflow import fairing\n",
    "        from kubeflow.fairing.kubernetes import utils as k8s_utils\n",
    "        DOCKER_REGISTRY = 'kubeflow-registry.default.svc.cluster.local:30000'\n",
    "        fairing.config.set_builder(\n",
    "            'append',\n",
    "            image_name='caltech-fairing-job',\n",
    "            base_image='brightfly/tf-fairing:2.0-gpu',\n",
    "            registry=DOCKER_REGISTRY,\n",
    "            push=True)\n",
    "        \n",
    "        fairing.config.set_deployer('job',\n",
    "                                    namespace='dudaji',\n",
    "                                    pod_spec_mutators=[\n",
    "                                    k8s_utils.mounting_pvc(pvc_name=\"caltech101\", \n",
    "                                                          pvc_mount_path=\"/result\")]\n",
    "                                    )\n",
    "        # python3\n",
    "        import IPython\n",
    "        ipy = IPython.get_ipython()\n",
    "        if ipy is None:\n",
    "            fairing.config.set_preprocessor('python', input_files=[__file__])        \n",
    "        fairing.config.run()\n",
    "    else:\n",
    "        train = Caltech101()\n",
    "        train.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
